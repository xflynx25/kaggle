{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where is it ?\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import TargetEncoder, StandardScaler\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_full = pd.read_csv('train.csv')#'/kaggle/input/playground-series-s4e9/train.csv')\n",
    "test_df_full = pd.read_csv('test.csv')#'/kaggle/input/playground-series-s4e9/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, HuberRegressor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge, HuberRegressor, LogisticRegression\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.ensemble import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class CustomTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, stats=['mean'], fill_value=None):\n",
    "        \"\"\"\n",
    "        stats: list of statistics for encoding, e.g., ['mean', 'median', 'mode']\n",
    "        fill_value: value to fill NaNs if categories are missing in the test set (defaults to global mean)\n",
    "        \"\"\"\n",
    "        self.stats = stats\n",
    "        self.fill_value = fill_value  # Value to fill NaNs (can be mean/median or a fixed number)\n",
    "        self.target_encodings_ = {stat: {} for stat in self.stats}\n",
    "        self.categorical_columns_ = None  # To store categorical column names for dropping later\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.categorical_columns_ = X.select_dtypes(include=['object', 'category']).columns  # Track categorical columns\n",
    "        # Iterate over categorical columns and encode them based on the selected statistic\n",
    "        for col in self.categorical_columns_:\n",
    "            for stat in self.stats:\n",
    "                if stat == 'mean':\n",
    "                    self.target_encodings_[stat][col] = y.groupby(X[col]).mean().to_dict()\n",
    "                elif stat == 'median':\n",
    "                    self.target_encodings_[stat][col] = y.groupby(X[col]).median().to_dict()\n",
    "                elif stat == 'mode':\n",
    "                    self.target_encodings_[stat][col] = y.groupby(X[col]).agg(lambda x: pd.Series.mode(x)[0]).to_dict()\n",
    "                elif stat == 'max':\n",
    "                    self.target_encodings_[stat][col] = y.groupby(X[col]).max().to_dict()\n",
    "                elif stat == 'min':\n",
    "                    self.target_encodings_[stat][col] = y.groupby(X[col]).min().to_dict()\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported statistic: {stat}\")\n",
    "        \n",
    "        # If fill_value is None, set it to the global mean of the target\n",
    "        if self.fill_value is None:\n",
    "            self.fill_value = y.mean()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_encoded = X.copy()\n",
    "        # Apply the target encodings for each statistic and concatenate as new features\n",
    "        for stat in self.stats:\n",
    "            for col, encodings in self.target_encodings_[stat].items():\n",
    "                new_col = f\"{col}_{stat}\"  # New column name for each encoding\n",
    "                X_encoded[new_col] = X_encoded[col].map(encodings)\n",
    "                \n",
    "                # Fill NaNs with the predefined fill_value\n",
    "                X_encoded[new_col] = X_encoded[new_col].fillna(self.fill_value)\n",
    "        \n",
    "        # Add isNaN columns\n",
    "        for col in self.categorical_columns_:\n",
    "            X_encoded[f\"{col}_isNaN\"] = X_encoded[col].isna().astype(int)\n",
    "\n",
    "        # Drop the original categorical columns\n",
    "        X_encoded = X_encoded.drop(columns=self.categorical_columns_)\n",
    "        \n",
    "        return X_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141400, 13)\n",
      "15344\n",
      "Softmax + Regression CV RMSE score: 73537.15959 ± 9964.19392\n",
      "Baseline Ridge CV RMSE score: 73618.54847 ± 9985.52258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Low Model Coefficients (mean):\", np.mean(coefficients_low, axis=0))\\nprint(\"Low Model Intercept (mean):\", np.mean(intercepts_low))\\nprint(\"High Model Coefficients (mean):\", np.mean(coefficients_high, axis=0))\\nprint(\"High Model Intercept (mean):\", np.mean(intercepts_high))\\n\\n# Output baseline coefficients and intercept\\nprint(\"Baseline Ridge Coefficients (mean):\", np.mean(baseline_coefficients, axis=0))\\nprint(\"Baseline Ridge Intercept (mean):\", np.mean(baseline_intercepts))\\n\\n# Output classifier feature importances\\nprint(\"Classifier Feature Importances (mean):\", np.mean(classifier_feature_importances, axis=0))\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "# preparing data, set tuning params\n",
    "SEED = 51\n",
    "DEV_FRACTION = .75\n",
    "df_dev = train_df_full.sample(frac=DEV_FRACTION, random_state=SEED)\n",
    "N = 5\n",
    "\n",
    "print(df_dev.shape)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_dev.drop(columns='price')\n",
    "y = df_dev['price']\n",
    "\n",
    "X['mileage'] = X.pop('milage') // 100  # binning mileage\n",
    "\n",
    "# Threshold for dividing the data into \"low\" and \"high\" price\n",
    "threshold = 75000\n",
    "\n",
    "# Binary target for classification (above or below the threshold)\n",
    "y_class = (y > threshold).astype(int)\n",
    "print(sum(y_class))\n",
    "\n",
    "# Define cross-validation\n",
    "kfold = RepeatedKFold(n_splits=10, n_repeats=1, random_state=SEED)\n",
    "\n",
    "# To store the RMSE results for softmax + regression and baseline\n",
    "rmse_scores = []\n",
    "baseline_rmse_scores = []\n",
    "\n",
    "# Store coefficients and intercepts for both high and low models\n",
    "coefficients_low = []\n",
    "coefficients_high = []\n",
    "intercepts_low = []\n",
    "intercepts_high = []\n",
    "\n",
    "# Store classifier feature importances (RandomForestClassifier does not have coefficients)\n",
    "classifier_feature_importances = []\n",
    "\n",
    "# Store Ridge coefficients and intercepts for baseline\n",
    "baseline_coefficients = []\n",
    "baseline_intercepts = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_reg, y_test_reg = y.iloc[train_index], y.iloc[test_index]\n",
    "    y_train_class = (y_train_reg > threshold).astype(int)\n",
    "\n",
    "    classifier = make_pipeline(\n",
    "        CustomTargetEncoder(stats=['mean']),  # Use multiple encoding strategies\n",
    "        StandardScaler().set_output(transform='pandas'),\n",
    "        CatBoostClassifier(\n",
    "            iterations=100, \n",
    "            depth=6, \n",
    "            learning_rate=0.1, \n",
    "            loss_function='Logloss', \n",
    "            verbose=0,  # Suppresses detailed output\n",
    "            random_state=SEED\n",
    "        )\n",
    "    )\n",
    "    classifier.fit(X_train, y_train_class)\n",
    "\n",
    "    # Predict probability of being in the high category\n",
    "    probs = classifier.predict_proba(X_test)[:, 1]  # Probability of being in the \"high\" category\n",
    "\n",
    "    # Collect feature importances from the classifier (RandomForestClassifier)\n",
    "    catboost_model = classifier.named_steps['catboostclassifier']\n",
    "    feature_importances = catboost_model.get_feature_importance()\n",
    "    classifier_feature_importances.append(feature_importances)\n",
    "\n",
    "    # Train two separate regression models for high and low categories\n",
    "    model_low = make_pipeline(\n",
    "        CustomTargetEncoder(stats=['mean']),  # Use multiple encoding strategies\n",
    "        StandardScaler().set_output(transform='pandas'),\n",
    "        Ridge(alpha=10)\n",
    "    )\n",
    "    model_high = make_pipeline(\n",
    "        CustomTargetEncoder(stats=['mean']),  # Use multiple encoding strategies\n",
    "        StandardScaler().set_output(transform='pandas'),\n",
    "        Ridge(alpha=10)\n",
    "    )\n",
    "\n",
    "    # Split training data into low and high categories based on threshold\n",
    "    X_train_low = X_train[y_train_reg <= threshold]\n",
    "    y_train_low = y_train_reg[y_train_reg <= threshold]\n",
    "    X_train_high = X_train[y_train_reg > threshold]\n",
    "    y_train_high = y_train_reg[y_train_reg > threshold]\n",
    "\n",
    "    # Train the low and high models\n",
    "    model_low.fit(X_train_low, y_train_low)\n",
    "    model_high.fit(X_train_high, y_train_high)\n",
    "\n",
    "    # Predict on the test set for both models\n",
    "    preds_low = model_low.predict(X_test)\n",
    "    preds_high = model_high.predict(X_test)\n",
    "\n",
    "    # Final prediction: weighted combination of low and high model predictions based on softmax probabilities\n",
    "    final_preds = probs * preds_high + (1 - probs) * preds_low\n",
    "\n",
    "    # Evaluate the performance using RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_reg, final_preds))\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "    # Collect coefficients and intercepts for both models\n",
    "    ridge_low = model_low.named_steps['ridge']\n",
    "    ridge_high = model_high.named_steps['ridge']\n",
    "\n",
    "    coefficients_low.append(ridge_low.coef_)\n",
    "    intercepts_low.append(ridge_low.intercept_)\n",
    "\n",
    "    coefficients_high.append(ridge_high.coef_)\n",
    "    intercepts_high.append(ridge_high.intercept_)\n",
    "\n",
    "    # Baseline Ridge regression model\n",
    "    baseline_model = make_pipeline(\n",
    "        CustomTargetEncoder(stats=['mean']),  # Use multiple encoding strategies\n",
    "        StandardScaler().set_output(transform='pandas'),\n",
    "        Ridge(alpha=10)\n",
    "    )\n",
    "    baseline_model.fit(X_train, y_train_reg)\n",
    "\n",
    "    # Predict on test set using baseline model\n",
    "    baseline_preds = baseline_model.predict(X_test)\n",
    "\n",
    "    # Evaluate baseline performance using RMSE\n",
    "    baseline_rmse = np.sqrt(mean_squared_error(y_test_reg, baseline_preds))\n",
    "    baseline_rmse_scores.append(baseline_rmse)\n",
    "\n",
    "    # Collect baseline Ridge coefficients and intercept\n",
    "    baseline_ridge = baseline_model.named_steps['ridge']\n",
    "    baseline_coefficients.append(baseline_ridge.coef_)\n",
    "    baseline_intercepts.append(baseline_ridge.intercept_)\n",
    "\n",
    "# Output the mean and standard deviation of the RMSE scores for softmax + regression and baseline\n",
    "print(f'Softmax + Regression CV RMSE score: {np.mean(rmse_scores):.5f} ± {np.std(rmse_scores):.5f}')\n",
    "print(f'Baseline Ridge CV RMSE score: {np.mean(baseline_rmse_scores):.5f} ± {np.std(baseline_rmse_scores):.5f}')\n",
    "\n",
    "# Output coefficients and intercepts for low and high models\n",
    "'''\n",
    "print(\"Low Model Coefficients (mean):\", np.mean(coefficients_low, axis=0))\n",
    "print(\"Low Model Intercept (mean):\", np.mean(intercepts_low))\n",
    "print(\"High Model Coefficients (mean):\", np.mean(coefficients_high, axis=0))\n",
    "print(\"High Model Intercept (mean):\", np.mean(intercepts_high))\n",
    "\n",
    "# Output baseline coefficients and intercept\n",
    "print(\"Baseline Ridge Coefficients (mean):\", np.mean(baseline_coefficients, axis=0))\n",
    "print(\"Baseline Ridge Intercept (mean):\", np.mean(baseline_intercepts))\n",
    "\n",
    "# Output classifier feature importances\n",
    "print(\"Classifier Feature Importances (mean):\", np.mean(classifier_feature_importances, axis=0))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75413, 13)\n",
      "8191\n",
      "Softmax + Regression CV RMSE score: 70801.51773 ± 12122.30700\n",
      "BASELINE CV RMSE score: 69074.71681 ± 12280.39675\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# preparing data, set tuning params\n",
    "SEED = 49\n",
    "DEV_FRACTION = .4\n",
    "df_dev = train_df_full.sample(frac=DEV_FRACTION, random_state=SEED)\n",
    "N = 5\n",
    "\n",
    "print(df_dev.shape)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_dev.drop(columns='price')\n",
    "y = df_dev['price']\n",
    "\n",
    "X['mileage'] = X.pop('milage') // 100  # binning mileage\n",
    "\n",
    "# Threshold for dividing the data into \"low\" and \"high\" price\n",
    "threshold = 75000\n",
    "\n",
    "# Binary target for classification (above or below the threshold)\n",
    "y_class = (y > threshold).astype(int)\n",
    "print(sum(y_class))\n",
    "\n",
    "# Define cross-validation\n",
    "kfold = RepeatedKFold(n_splits=10, n_repeats=2, random_state=SEED)\n",
    "\n",
    "# To store the RMSE results for softmax + regression\n",
    "rmse_scores = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_reg, y_test_reg = y.iloc[train_index], y.iloc[test_index]\n",
    "    y_train_class = (y_train_reg > threshold).astype(int)\n",
    "\n",
    "    # Classifier to predict high/low category (Softmax)\n",
    "    classifier = make_pipeline(\n",
    "        #TargetEncoder(random_state=0, target_type='continuous').set_output(transform='pandas'),\n",
    "        CustomTargetEncoder(stats=['mean']),  # Use multiple encoding strategies\n",
    "        StandardScaler().set_output(transform='pandas'),\n",
    "        RandomForestClassifier(random_state=SEED)\n",
    "    )\n",
    "    classifier.fit(X_train, y_train_class)\n",
    "\n",
    "    # Predict probability of being in the high category\n",
    "    probs = classifier.predict_proba(X_test)[:, 1]  # Probability of being in the \"high\" category\n",
    "\n",
    "    # Train two separate regression models for high and low categories\n",
    "    model_low = make_pipeline(\n",
    "        #TargetEncoder(random_state=0, target_type='continuous').set_output(transform='pandas'),\n",
    "        CustomTargetEncoder(stats=['mean']),  # Use multiple encoding strategies\n",
    "        StandardScaler().set_output(transform='pandas'),\n",
    "        Ridge(alpha=10)\n",
    "    )\n",
    "    model_high = make_pipeline(\n",
    "        #TargetEncoder(random_state=0, target_type='continuous').set_output(transform='pandas'),\n",
    "        CustomTargetEncoder(stats=['mean']),  # Use multiple encoding strategies\n",
    "        StandardScaler().set_output(transform='pandas'),\n",
    "        Ridge(alpha=10)\n",
    "    )\n",
    "\n",
    "    # Split training data into low and high categories based on threshold\n",
    "    X_train_low = X_train[y_train_reg <= threshold]\n",
    "    y_train_low = y_train_reg[y_train_reg <= threshold]\n",
    "    X_train_high = X_train[y_train_reg > threshold]\n",
    "    y_train_high = y_train_reg[y_train_reg > threshold]\n",
    "\n",
    "    # Train the low and high models\n",
    "    model_low.fit(X_train_low, y_train_low)\n",
    "    model_high.fit(X_train_high, y_train_high)\n",
    "\n",
    "    # Predict on the test set for both models\n",
    "    preds_low = model_low.predict(X_test)\n",
    "    preds_high = model_high.predict(X_test)\n",
    "\n",
    "    # Final prediction: weighted combination of low and high model predictions based on softmax probabilities\n",
    "    final_preds = probs * preds_high + (1 - probs) * preds_low\n",
    "\n",
    "    # Evaluate the performance using RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_reg, final_preds))\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "# Output the mean and standard deviation of the RMSE scores\n",
    "print(f'Softmax + Regression CV RMSE score: {np.mean(rmse_scores):.5f} ± {np.std(rmse_scores):.5f}')\n",
    "\n",
    "# Perform baseline cross-validation\n",
    "scoring = 'neg_root_mean_squared_error'\n",
    "pipeline = make_pipeline(\n",
    "    TargetEncoder(random_state=0, target_type='continuous').set_output(transform='pandas'),\n",
    "    StandardScaler().set_output(transform='pandas'),\n",
    "    Ridge(alpha=10)\n",
    ")\n",
    "\n",
    "# Perform cross-validation for baseline\n",
    "baseline_scores = -cross_val_score(pipeline, X, y, scoring=scoring, cv=kfold, n_jobs=-1)\n",
    "\n",
    "print(f'BASELINE CV RMSE score: {np.mean(baseline_scores):.5f} ± {np.std(baseline_scores):.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Ridge' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m mod \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mridge\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRidge Coefficients: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmod\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRidge Intercept (Bias): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmod\u001b[38;5;241m.\u001b[39mintercept_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m mod \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mridge\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Ridge' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "mod = pipeline.named_steps['ridge']\n",
    "print(f'Ridge Coefficients: {mod.coef_}')\n",
    "print(f'Ridge Intercept (Bias): {mod.intercept_}')\n",
    "\n",
    "mod = classifier.named_steps['ridge']\n",
    "print(f'Ridge Coefficients: {mod.coef_}')\n",
    "print(f'Ridge Intercept (Bias): {mod.intercept_}')\n",
    "\n",
    "mod = model_high.named_steps['ridge']\n",
    "print(f'Ridge Coefficients: {mod.coef_}')\n",
    "print(f'Ridge Intercept (Bias): {mod.intercept_}')\n",
    "\n",
    "mod = model_low.named_steps['ridge']\n",
    "print(f'Ridge Coefficients: {mod.coef_}')\n",
    "print(f'Ridge Intercept (Bias): {mod.intercept_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37707, 13)\n",
      "Final RMSE: 71445.81245\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, HuberRegressor, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# preparing data, set tuning params\n",
    "SEED = 2\n",
    "DEV_FRACTION = .2\n",
    "df_dev = train_df_full.sample(frac=DEV_FRACTION, random_state=SEED)\n",
    "N = 5\n",
    "test_frac = 0.2\n",
    "\n",
    "print(df_dev.shape)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_dev.drop(columns='price')\n",
    "y = df_dev['price']\n",
    "\n",
    "X['mileage'] = X.pop('milage')//100 # binning mileage\n",
    "\n",
    "\n",
    "# Threshold for dividing the data into \"low\" and \"high\" price\n",
    "threshold = y.median()\n",
    "\n",
    "# Binary target for classification (above or below the threshold)\n",
    "y_class = (y > threshold).astype(int)\n",
    "\n",
    "# Split data for training classifier and regression models\n",
    "X_train, X_test, y_train_class, y_test_class, y_train_reg, y_test_reg = train_test_split(X, y_class, y, test_size=test_frac, random_state=SEED)\n",
    "\n",
    "# Classifier to predict high/low category (Softmax)\n",
    "classifier = make_pipeline(\n",
    "    TargetEncoder(random_state=0, target_type='continuous').set_output(transform='pandas'),\n",
    "    StandardScaler().set_output(transform='pandas'),\n",
    "    RandomForestClassifier(random_state=SEED)\n",
    ")\n",
    "classifier.fit(X_train, y_train_class)\n",
    "\n",
    "# Predict probability of being in the high category\n",
    "probs = classifier.predict_proba(X_test)[:, 1]  # Probability of being in the \"high\" category\n",
    "\n",
    "# Train two separate regression models for high and low categories\n",
    "model_low = make_pipeline(\n",
    "    TargetEncoder(random_state=0, target_type='continuous').set_output(transform='pandas'),\n",
    "    StandardScaler().set_output(transform='pandas'),\n",
    "    Ridge(alpha=10)\n",
    ")\n",
    "model_high = make_pipeline(\n",
    "    TargetEncoder(random_state=0, target_type='continuous').set_output(transform='pandas'),\n",
    "    StandardScaler().set_output(transform='pandas'),\n",
    "    Ridge(alpha=10)\n",
    ")\n",
    "\n",
    "# Split training data into low and high categories based on threshold\n",
    "X_train_low = X_train[y_train_reg <= threshold]\n",
    "y_train_low = y_train_reg[y_train_reg <= threshold]\n",
    "X_train_high = X_train[y_train_reg > threshold]\n",
    "y_train_high = y_train_reg[y_train_reg > threshold]\n",
    "\n",
    "# Train the low and high models\n",
    "model_low.fit(X_train_low, y_train_low)\n",
    "model_high.fit(X_train_high, y_train_high)\n",
    "\n",
    "# Predict on the test set for both models\n",
    "preds_low = model_low.predict(X_test)\n",
    "preds_high = model_high.predict(X_test)\n",
    "\n",
    "# Final prediction: weighted combination of low and high model predictions based on softmax probabilities\n",
    "final_preds = probs * preds_high + (1 - probs) * preds_low\n",
    "\n",
    "# Evaluate the performance (you can use any metric like RMSE)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(y_test_reg, final_preds))\n",
    "print(f'Final RMSE: {rmse:.5f}')\n",
    "\n",
    "\n",
    "\n",
    "# compare against baseline\n",
    "kfold = RepeatedKFold(n_splits=5, n_repeats=1, random_state=SEED)\n",
    "scoring = 'neg_root_mean_squared_error'\n",
    "pipeline = make_pipeline(\n",
    "    TargetEncoder(random_state=0, target_type='continuous').set_output(transform='pandas'),\n",
    "    StandardScaler().set_output(transform='pandas'),\n",
    "    Ridge(alpha=10)\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = -cross_val_score(pipeline, X, y, scoring=scoring, cv=kfold, n_jobs=-1)\n",
    "\n",
    "print(f'BASELINE CV RMSE score: {np.mean(scores):.5f} ± {np.std(scores):.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94266, 13)\n",
      "Evaluating Ridge...\n",
      "Ridge CV RMSE score: 75968.10291 ± 8323.04778\n",
      "Ridge CV RMSE score: 75542.65268 ± 8367.63320\n",
      "Ridge CV RMSE score: 76626.62285 ± 8243.98983\n",
      "Ridge CV RMSE score: 74918.82361 ± 8357.89014\n",
      "Evaluating HuberRegressor...\n",
      "HuberRegressor CV RMSE score: 76579.93232 ± 8307.39045\n",
      "HuberRegressor CV RMSE score: 76607.68289 ± 8318.64065\n",
      "HuberRegressor CV RMSE score: 76506.42278 ± 8327.45015\n",
      "HuberRegressor CV RMSE score: 76170.35266 ± 8280.37124\n",
      "Evaluating RANSACRegressor...\n",
      "RANSACRegressor CV RMSE score: 77429.08037 ± 8091.46159\n",
      "RANSACRegressor CV RMSE score: 77799.80756 ± 7974.10525\n",
      "RANSACRegressor CV RMSE score: 84382.58241 ± 10407.66583\n",
      "RANSACRegressor CV RMSE score: 77832.36453 ± 7956.82316\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# preparing data, set tuning params\n",
    "SEED = 2\n",
    "DEV_FRACTION = .5\n",
    "df_dev = train_df_full.sample(frac=DEV_FRACTION, random_state=SEED)\n",
    "N = 5\n",
    "test_frac = 0.2\n",
    "\n",
    "print(df_dev.shape)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_dev.drop(columns='price')\n",
    "y = df_dev['price']\n",
    "\n",
    "X['mileage'] = X.pop('milage')//100 # binning mileage\n",
    "\n",
    "\n",
    "# Defining models with different outlier resistance\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=10),\n",
    "    'HuberRegressor': HuberRegressor(),\n",
    "    'RANSACRegressor': RANSACRegressor()\n",
    "}\n",
    "\n",
    "# Define cross-validation\n",
    "kfold = RepeatedKFold(n_splits=5, n_repeats=1, random_state=SEED)\n",
    "scoring = 'neg_root_mean_squared_error'\n",
    "encoding_stats = ['mean']\n",
    "ENCODERS = [\n",
    "            CustomTargetEncoder(stats=['median']),  # Use multiple encoding strategies\n",
    "            TargetEncoder(random_state=0, target_type='continuous').set_output(transform='pandas'),\n",
    "]\n",
    "# Loop over each model\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    for encoder in ENCODERS:\n",
    "        pipeline = make_pipeline(\n",
    "            encoder, \n",
    "            StandardScaler().set_output(transform='pandas'),\n",
    "            model\n",
    "        )\n",
    "\n",
    "        # Perform cross-validation\n",
    "        scores = -cross_val_score(pipeline, X, y, scoring=scoring, cv=kfold, n_jobs=-1)\n",
    "        \n",
    "        print(f'{name} CV RMSE score: {np.mean(scores):.5f} ± {np.std(scores):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Ridge...\n",
      "Ridge CV RMSE score: 89529.24871 ± 19893.50379\n",
      "Evaluating HuberRegressor...\n",
      "HuberRegressor CV RMSE score: 90699.35000 ± 19751.27117\n",
      "Evaluating RANSACRegressor...\n",
      "RANSACRegressor CV RMSE score: 91900.05487 ± 19493.58000\n",
      "Evaluating LogisticRegression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jflyn/anaconda3/envs/kaggle/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression CV RMSE score: 119875.49289 ± 11898.71066\n",
      "Evaluating Blended_Ridge_Huber...\n",
      "Blended_Ridge_Huber CV RMSE score: 89586.99658 ± 19810.40579\n"
     ]
    }
   ],
   "source": [
    "# Defining models with different outlier resistance + logistic regression\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'HuberRegressor': HuberRegressor(),\n",
    "    'RANSACRegressor': RANSACRegressor(),\n",
    "    'LogisticRegression': LogisticRegression()  # Logistic baseline for comparison\n",
    "}\n",
    "\n",
    "# Blended model (Stacking/Blending approach)\n",
    "# Blend Ridge and Huber\n",
    "blended_model = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('ridge', Ridge(alpha=1.0)),\n",
    "        ('huber', HuberRegressor())\n",
    "    ],\n",
    "    final_estimator=Ridge()\n",
    ")\n",
    "models['Blended_Ridge_Huber'] = blended_model\n",
    "\n",
    "# Define cross-validation\n",
    "kfold = RepeatedKFold(n_splits=5, n_repeats=1, random_state=SEED)\n",
    "scoring = 'neg_root_mean_squared_error'\n",
    "\n",
    "# Loop over each model\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    pipeline = make_pipeline(\n",
    "        TargetEncoder(random_state=0, target_type='continuous').set_output(transform='pandas'),\n",
    "        StandardScaler().set_output(transform='pandas'),\n",
    "        model\n",
    "    )\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores = -cross_val_score(pipeline, X, y, scoring=scoring, cv=kfold, n_jobs=-1)\n",
    "    \n",
    "    print(f'{name} CV RMSE score: {np.mean(scores):.5f} ± {np.std(scores):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Ridge...\n",
      "Ridge CV RMSE score: 89529.24871 ± 19893.50379\n",
      "Evaluating HuberRegressor...\n",
      "HuberRegressor CV RMSE score: 90699.35000 ± 19751.27117\n",
      "Evaluating RandomForest...\n",
      "RandomForest CV RMSE score: 93587.21386 ± 18824.94566\n",
      "Evaluating GradientBoosting...\n",
      "GradientBoosting CV RMSE score: 91172.40501 ± 19493.14176\n",
      "Evaluating Blended_Linear_Trees_1...\n",
      "Blended_Linear_Trees_1 CV RMSE score: 92391.51531 ± 20424.49250\n",
      "Evaluating Blended_Trees_Only...\n",
      "Blended_Trees_Only CV RMSE score: 91255.51729 ± 19634.22516\n"
     ]
    }
   ],
   "source": [
    "# Defining models with different outlier resistance\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'HuberRegressor': HuberRegressor(),\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=SEED),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=SEED)\n",
    "}\n",
    "\n",
    "# Blended model: Combining linear and tree models\n",
    "blended_model_1 = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('ridge', Ridge(alpha=1.0)),\n",
    "        ('huber', HuberRegressor()),\n",
    "        ('random_forest', RandomForestRegressor(n_estimators=100, random_state=SEED))\n",
    "    ],\n",
    "    final_estimator=GradientBoostingRegressor(random_state=SEED)\n",
    ")\n",
    "models['Blended_Linear_Trees_1'] = blended_model_1\n",
    "\n",
    "# Another blend: More focus on tree models\n",
    "blended_model_2 = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('random_forest', RandomForestRegressor(n_estimators=100, random_state=SEED)),\n",
    "        ('gradient_boosting', GradientBoostingRegressor(random_state=SEED))\n",
    "    ],\n",
    "    final_estimator=HuberRegressor()\n",
    ")\n",
    "models['Blended_Trees_Only'] = blended_model_2\n",
    "\n",
    "# Define cross-validation\n",
    "kfold = RepeatedKFold(n_splits=5, n_repeats=1, random_state=SEED)\n",
    "scoring = 'neg_root_mean_squared_error'\n",
    "\n",
    "# Loop over each model\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    pipeline = make_pipeline(\n",
    "        TargetEncoder(random_state=0, target_type='continuous').set_output(transform='pandas'),\n",
    "        StandardScaler().set_output(transform='pandas'),\n",
    "        model\n",
    "    )\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores = -cross_val_score(pipeline, X, y, scoring=scoring, cv=kfold, n_jobs=-1)\n",
    "    \n",
    "    print(f'{name} CV RMSE score: {np.mean(scores):.5f} ± {np.std(scores):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Separate features (X) and target (y) from the full training set\n",
    "X_train = train_df_full.drop(columns='price')\n",
    "y_train = train_df_full['price']\n",
    "X_test = test_df_full\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "threshold = 50000\n",
    "y_train_class = (y_train > threshold).astype(int)\n",
    "\n",
    "classifier = make_pipeline(\n",
    "    CustomTargetEncoder(stats=['mean']),  # Use multiple encoding strategies\n",
    "    StandardScaler().set_output(transform='pandas'),\n",
    "    CatBoostClassifier(\n",
    "        iterations=100, \n",
    "        depth=6, \n",
    "        learning_rate=0.1, \n",
    "        loss_function='Logloss', \n",
    "        verbose=0,  # Suppresses detailed output\n",
    "        random_state=SEED\n",
    "    )\n",
    ")\n",
    "classifier.fit(X_train, y_train_class)\n",
    "\n",
    "# Predict probability of being in the high category\n",
    "probs = classifier.predict_proba(X_test)[:, 1]  # Probability of being in the \"high\" category\n",
    "\n",
    "# Collect feature importances from the classifier (RandomForestClassifier)\n",
    "catboost_model = classifier.named_steps['catboostclassifier']\n",
    "feature_importances = catboost_model.get_feature_importance()\n",
    "classifier_feature_importances.append(feature_importances)\n",
    "\n",
    "# Train two separate regression models for high and low categories\n",
    "model_low = make_pipeline(\n",
    "    CustomTargetEncoder(stats=['mean']),  # Use multiple encoding strategies\n",
    "    StandardScaler().set_output(transform='pandas'),\n",
    "    Ridge(alpha=10)\n",
    ")\n",
    "model_high = make_pipeline(\n",
    "    CustomTargetEncoder(stats=['mean']),  # Use multiple encoding strategies\n",
    "    StandardScaler().set_output(transform='pandas'),\n",
    "    Ridge(alpha=10)\n",
    ")\n",
    "\n",
    "# Split training data into low and high categories based on threshold\n",
    "X_train_low = X_train[y_train <= threshold]\n",
    "y_train_low = y_train[y_train <= threshold]\n",
    "X_train_high = X_train[y_train > threshold]\n",
    "y_train_high = y_train[y_train > threshold]\n",
    "\n",
    "# Train the low and high models\n",
    "model_low.fit(X_train_low, y_train_low)\n",
    "model_high.fit(X_train_high, y_train_high)\n",
    "\n",
    "# Predict on the test set for both models\n",
    "preds_low = model_low.predict(X_test)\n",
    "preds_high = model_high.predict(X_test)\n",
    "\n",
    "# Final prediction: weighted combination of low and high model predictions based on softmax probabilities\n",
    "final_preds = probs * preds_high + (1 - probs) * preds_low\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Create a submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df_full['id'],  # Ensure the 'id' from the test set is preserved\n",
    "    'price': final_preds       # The predicted prices\n",
    "})\n",
    "\n",
    "# Step 6: Save the submission to a CSV file\n",
    "submission.to_csv('submission6_outlier2stage.csv', index=False)  # Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
