{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where is it ?\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import TargetEncoder, StandardScaler\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_full = pd.read_csv('train.csv')#'/kaggle/input/playground-series-s4e9/train.csv')\n",
    "test_df_full = pd.read_csv('test.csv')#'/kaggle/input/playground-series-s4e9/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188533, 13)\n",
      "A fold\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 177\u001b[0m\n\u001b[1;32m    173\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Run cross-validation\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m cross_validate(X, y, target_encoder, scaler, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 143\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(X, y, target_encoder, scaler, n_splits, n_repeats, epochs, batch_size)\u001b[0m\n\u001b[1;32m    140\u001b[0m X_test_scaled, _ \u001b[38;5;241m=\u001b[39m preprocess_data(X_test, target_encoder\u001b[38;5;241m=\u001b[39mtarget_encoder, scaler\u001b[38;5;241m=\u001b[39mscaler, nan_columns\u001b[38;5;241m=\u001b[39mnan_columns)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Train and evaluate PyTorch model, printing the training/validation loss\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m model \u001b[38;5;241m=\u001b[39m train_pytorch_model(X_train_scaled, y_train, X_test_scaled, y_test, input_size\u001b[38;5;241m=\u001b[39mX_train_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m    144\u001b[0m rmse_nn \u001b[38;5;241m=\u001b[39m evaluate_pytorch_model(model, X_test_scaled, y_test)\n\u001b[1;32m    145\u001b[0m rmse_scores_nn\u001b[38;5;241m.\u001b[39mappend(rmse_nn)\n",
      "Cell \u001b[0;32mIn[4], line 107\u001b[0m, in \u001b[0;36mtrain_pytorch_model\u001b[0;34m(X_train, y_train, X_test, y_test, input_size, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    106\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 107\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(batch_X)\n\u001b[1;32m    108\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[1;32m    109\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m, in \u001b[0;36mSimpleNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m---> 25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/torch/nn/modules/module.py:1549\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1546\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1551\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define PyTorch model with more capacity\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)  # Increased to 128 neurons\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)  # Increased second layer to 64 neurons\n",
    "        self.fc3 = nn.Linear(64, 1)  # Output layer remains the same\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout to prevent overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc3(x)\n",
    "    \n",
    "\n",
    "# Function to preprocess data and handle NaNs\n",
    "def preprocess_data(X, y=None, target_encoder=None, scaler=None, nan_columns=None):\n",
    "    # Remove 'id' column if it exists\n",
    "    if 'id' in X.columns:\n",
    "        X = X.drop(columns=['id'])\n",
    "    \n",
    "    # If nan_columns is provided (during test), use it; otherwise detect from the data (during train)\n",
    "    if nan_columns is None:\n",
    "        # Training phase: Identify columns with NaNs and store them\n",
    "        nan_columns = X.columns[X.isna().any()].tolist()\n",
    "    \n",
    "    # Handle missing values (NaNs) by adding is_nan columns and imputing\n",
    "    for col in nan_columns:\n",
    "        X[f'{col}_is_nan'] = X[col].isna().astype(int)  # Add 'is_nan' column\n",
    "        # this be done automatically i think, and we want consistent if new one is found later \n",
    "        #X[col].fillna(X[col].mean(), inplace=True)  # Impute NaNs with the column mean (or other strategy)\n",
    "\n",
    "    # Apply TargetEncoder and StandardScaler\n",
    "    if target_encoder is not None:\n",
    "        if y is not None:\n",
    "            X = target_encoder.fit_transform(X, y)\n",
    "        else:\n",
    "            X = target_encoder.transform(X)\n",
    "    if scaler is not None:\n",
    "        if y is not None:\n",
    "            X = scaler.fit_transform(X)\n",
    "        else:\n",
    "            X = scaler.transform(X)\n",
    "    \n",
    "    return X, nan_columns\n",
    "\n",
    "\n",
    "# Function to evaluate PyTorch model\n",
    "def evaluate_pytorch_model(model, X_test, y_test):\n",
    "    # Convert test data to PyTorch tensors\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor)#.squeeze()  # Use squeeze to flatten the prediction shape\n",
    "        rmse = torch.sqrt(torch.mean((predictions - y_test_tensor) ** 2)).item()\n",
    "    return rmse\n",
    "\n",
    "# Function to train and evaluate Ridge regression\n",
    "def train_and_evaluate_ridge(X_train, y_train, X_test, y_test):\n",
    "    ridge_model = Ridge(alpha=1.0)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    predictions = ridge_model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    return rmse\n",
    "\n",
    "# Function to train PyTorch model and print training/validation loss at each epoch\n",
    "def train_pytorch_model(X_train, y_train, X_test, y_test, input_size, epochs=20, batch_size=64, learning_rate=0.001):\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize PyTorch model, loss, and optimizer\n",
    "    model = SimpleNN(input_size=input_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model and track losses\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch_X.size(0)  # Accumulate training loss\n",
    "\n",
    "        # Calculate average training loss\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Validation loss (RMSE) after each epoch\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_test_tensor)#.squeeze()  # Use squeeze to flatten the prediction shap\n",
    "            val_loss = torch.sqrt(torch.mean((predictions - y_test_tensor) ** 2)).item()  # RMSE\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {train_loss:.0f}, Validation RMSE: {val_loss:.0f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function for cross-validation loop (with printing training/validation errors)\n",
    "def cross_validate(X, y, target_encoder, scaler, n_splits=5, n_repeats=1, epochs=20, batch_size=64):\n",
    "    kfold = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=SEED)\n",
    "    rmse_scores_nn = []\n",
    "    rmse_scores_ridge = []\n",
    "\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        print('A fold')\n",
    "        # Split the data\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Preprocess the data\n",
    "        X_train_scaled, nan_columns = preprocess_data(X_train, y_train, target_encoder, scaler)\n",
    "        X_test_scaled, _ = preprocess_data(X_test, target_encoder=target_encoder, scaler=scaler, nan_columns=nan_columns)\n",
    "\n",
    "        # Train and evaluate PyTorch model, printing the training/validation loss\n",
    "        model = train_pytorch_model(X_train_scaled, y_train, X_test_scaled, y_test, input_size=X_train_scaled.shape[1], epochs=epochs, batch_size=batch_size)\n",
    "        rmse_nn = evaluate_pytorch_model(model, X_test_scaled, y_test)\n",
    "        rmse_scores_nn.append(rmse_nn)\n",
    "\n",
    "        # Train and evaluate Ridge regression\n",
    "        rmse_ridge = train_and_evaluate_ridge(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "        rmse_scores_ridge.append(rmse_ridge)\n",
    "\n",
    "    print(f'PyTorch Neural Network CV RMSE score: {np.mean(rmse_scores_nn):.0f} ± {np.std(rmse_scores_nn):.0f}')\n",
    "    print(f'Ridge Regression CV RMSE score: {np.mean(rmse_scores_ridge):.0f} ± {np.std(rmse_scores_ridge):.0f}')\n",
    "\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "SEED = 5\n",
    "DEV_FRACTION = 1\n",
    "df_dev = train_df_full.sample(frac=DEV_FRACTION, random_state=SEED)\n",
    "\n",
    "print(df_dev.shape)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_dev.drop(columns='price')\n",
    "y = df_dev['price']\n",
    "\n",
    "\n",
    "# Cross-validation setup\n",
    "kfold = RepeatedKFold(n_splits=5, n_repeats=1, random_state=SEED)\n",
    "\n",
    "# Initialize TargetEncoder and StandardScaler\n",
    "target_encoder = TargetEncoder(random_state=SEED, target_type='continuous').set_output(transform='pandas')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Run cross-validation\n",
    "cross_validate(X, y, target_encoder, scaler, epochs = 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# producing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188533, 14)\n",
      "(125690, 14)\n"
     ]
    }
   ],
   "source": [
    "# Main function to train on full data and make a submission\n",
    "def train_full_model_and_submit(train_df, test_df, target_encoder, scaler, epochs=20, batch_size=64):\n",
    "    # Separate features and target\n",
    "    X_train = train_df.drop(columns='price')\n",
    "    y_train = train_df['price']\n",
    "    X_test = test_df\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train_scaled, nan_columns = preprocess_data(X_train, y_train, target_encoder, scaler)\n",
    "\n",
    "    # Train the PyTorch model on the full training data\n",
    "    model = train_pytorch_model(X_train_scaled, y_train, input_size=X_train_scaled.shape[1], epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    # Prepare the test data and make predictions\n",
    "    X_test_scaled, _ = preprocess_data(X_test, target_encoder=target_encoder, scaler=scaler, nan_columns=nan_columns)\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor).numpy().flatten()\n",
    "\n",
    "    # Create submission DataFrame\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'price': predictions\n",
    "    })\n",
    "\n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('submission10_Blended.csv', index=False)\n",
    "\n",
    "\n",
    "# Train the full model and create a submission\n",
    "train_full_model_and_submit(train_df_full, test_df_full, target_encoder, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                  0\n",
       "brand               0\n",
       "model               0\n",
       "model_year          0\n",
       "milage              0\n",
       "fuel_type        5083\n",
       "engine              0\n",
       "transmission        0\n",
       "ext_col             0\n",
       "int_col             0\n",
       "accident         2452\n",
       "clean_title     21419\n",
       "price               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_full.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                  0\n",
       "brand               0\n",
       "model               0\n",
       "model_year          0\n",
       "milage              0\n",
       "fuel_type        3383\n",
       "engine              0\n",
       "transmission        0\n",
       "ext_col             0\n",
       "int_col             0\n",
       "accident         1632\n",
       "clean_title     14239\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_full.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18853, 13)\n",
      "a fold\n",
      "  epoch       train_loss       valid_loss     dur\n",
      "-------  ---------------  ---------------  ------\n",
      "      1  \u001b[36m7937223691.4792\u001b[0m  \u001b[32m9673196129.2410\u001b[0m  0.1585\n",
      "      2  \u001b[36m7756225891.9705\u001b[0m  \u001b[32m9285550054.5442\u001b[0m  0.1543\n",
      "      3  \u001b[36m7137827504.8235\u001b[0m  \u001b[32m8461583321.3073\u001b[0m  0.1530\n",
      "      4  \u001b[36m6360444649.1902\u001b[0m  \u001b[32m7761897463.0056\u001b[0m  0.1549\n",
      "      5  \u001b[36m5910015679.2255\u001b[0m  \u001b[32m7440568706.2486\u001b[0m  0.1530\n",
      "      6  \u001b[36m5702013150.7455\u001b[0m  \u001b[32m7294467748.4441\u001b[0m  0.1525\n",
      "      7  \u001b[36m5582267252.6375\u001b[0m  \u001b[32m7206747142.9579\u001b[0m  0.1525\n",
      "      8  \u001b[36m5536552714.1212\u001b[0m  \u001b[32m7150708266.2565\u001b[0m  0.1533\n",
      "      9  \u001b[36m5482051016.8481\u001b[0m  \u001b[32m7109609947.8528\u001b[0m  0.1515\n",
      "     10  \u001b[36m5446911179.4845\u001b[0m  \u001b[32m7079921701.3351\u001b[0m  0.1535\n",
      "     11  \u001b[36m5411764544.3395\u001b[0m  \u001b[32m7056994434.3334\u001b[0m  0.1558\n",
      "     12  \u001b[36m5384878958.3092\u001b[0m  \u001b[32m7041607920.9811\u001b[0m  0.1530\n",
      "     13  \u001b[36m5360365606.3151\u001b[0m  \u001b[32m7028960992.5197\u001b[0m  0.1543\n",
      "     14  5364115970.6841  \u001b[32m7020504671.2894\u001b[0m  0.1521\n",
      "     15  \u001b[36m5357649046.7567\u001b[0m  \u001b[32m7014822946.6198\u001b[0m  0.1562\n",
      "     16  \u001b[36m5352735773.7058\u001b[0m  \u001b[32m7008527906.7895\u001b[0m  0.1553\n",
      "     17  \u001b[36m5310024244.1071\u001b[0m  \u001b[32m7002930339.7653\u001b[0m  0.1598\n",
      "     18  5326788596.1973  \u001b[32m6999514146.1956\u001b[0m  0.1721\n",
      "     19  5326616532.5076  \u001b[32m6996227434.1505\u001b[0m  0.1534\n",
      "a fold\n",
      "  epoch       train_loss       valid_loss     dur\n",
      "-------  ---------------  ---------------  ------\n",
      "      1  \u001b[36m8096455432.5086\u001b[0m  \u001b[32m7511530665.5777\u001b[0m  0.1559\n",
      "      2  \u001b[36m7917424426.7763\u001b[0m  \u001b[32m7140491670.3586\u001b[0m  0.1538\n",
      "      3  \u001b[36m7302615281.7412\u001b[0m  \u001b[32m6330464608.3924\u001b[0m  0.1563\n",
      "      4  \u001b[36m6470875925.9186\u001b[0m  \u001b[32m5596562740.7783\u001b[0m  0.1513\n",
      "      5  \u001b[36m5957179879.9914\u001b[0m  \u001b[32m5263907778.4819\u001b[0m  0.1532\n",
      "      6  \u001b[36m5771096979.3406\u001b[0m  \u001b[32m5132715007.7454\u001b[0m  0.1491\n",
      "      7  \u001b[36m5676707180.3093\u001b[0m  \u001b[32m5057902214.9155\u001b[0m  0.1527\n",
      "      8  \u001b[36m5605601876.0143\u001b[0m  \u001b[32m5005851348.3010\u001b[0m  0.1480\n",
      "      9  \u001b[36m5559926542.6619\u001b[0m  \u001b[32m4968248433.3629\u001b[0m  0.1510\n",
      "     10  \u001b[36m5547221242.6317\u001b[0m  \u001b[32m4941063546.9513\u001b[0m  0.1506\n",
      "     11  \u001b[36m5543539955.6297\u001b[0m  \u001b[32m4920703628.0066\u001b[0m  0.1504\n",
      "     12  \u001b[36m5511908579.8432\u001b[0m  \u001b[32m4904861042.6357\u001b[0m  0.1514\n",
      "     13  \u001b[36m5496193865.3732\u001b[0m  \u001b[32m4893791205.0169\u001b[0m  0.1510\n",
      "     14  \u001b[36m5466784750.4736\u001b[0m  \u001b[32m4884286072.4906\u001b[0m  0.1526\n",
      "     15  \u001b[36m5463035630.0174\u001b[0m  \u001b[32m4876760546.6410\u001b[0m  0.1537\n",
      "     16  \u001b[36m5455209162.8585\u001b[0m  \u001b[32m4871127804.2665\u001b[0m  0.1506\n",
      "     17  \u001b[36m5451333572.8855\u001b[0m  \u001b[32m4866915605.4677\u001b[0m  0.1485\n",
      "     18  \u001b[36m5441836815.7175\u001b[0m  \u001b[32m4861845455.9311\u001b[0m  0.1516\n",
      "     19  \u001b[36m5440721729.8089\u001b[0m  \u001b[32m4858625545.6732\u001b[0m  0.1504\n",
      "     20  \u001b[36m5431020615.6970\u001b[0m  \u001b[32m4855006690.1743\u001b[0m  0.1526\n",
      "a fold\n",
      "  epoch       train_loss       valid_loss     dur\n",
      "-------  ---------------  ---------------  ------\n",
      "      1  \u001b[36m7935186376.8746\u001b[0m  \u001b[32m7152915442.3812\u001b[0m  0.1530\n",
      "      2  \u001b[36m7747907914.5402\u001b[0m  \u001b[32m6753564497.6705\u001b[0m  0.1513\n",
      "      3  \u001b[36m7091439126.3536\u001b[0m  \u001b[32m5902966954.5747\u001b[0m  0.1507\n",
      "      4  \u001b[36m6280835731.4254\u001b[0m  \u001b[32m5237527042.2168\u001b[0m  0.1512\n",
      "      5  \u001b[36m5821603168.1936\u001b[0m  \u001b[32m4983911558.0351\u001b[0m  0.1503\n",
      "      6  \u001b[36m5659894742.0937\u001b[0m  \u001b[32m4884461825.2622\u001b[0m  0.1493\n",
      "      7  \u001b[36m5544642124.1422\u001b[0m  \u001b[32m4820965835.6884\u001b[0m  0.1692\n",
      "      8  \u001b[36m5498406800.9641\u001b[0m  \u001b[32m4773497251.8078\u001b[0m  0.1489\n",
      "      9  \u001b[36m5480486896.6803\u001b[0m  \u001b[32m4737367307.5824\u001b[0m  0.1502\n",
      "     10  \u001b[36m5431706563.9307\u001b[0m  \u001b[32m4709571018.6596\u001b[0m  0.1509\n",
      "     11  \u001b[36m5383920046.0545\u001b[0m  \u001b[32m4690171501.0991\u001b[0m  0.1495\n",
      "     12  \u001b[36m5360700122.8678\u001b[0m  \u001b[32m4676230095.9841\u001b[0m  0.1500\n",
      "     13  5369847352.1227  \u001b[32m4665414028.9824\u001b[0m  0.1509\n",
      "     14  \u001b[36m5348372025.0138\u001b[0m  \u001b[32m4658579858.1584\u001b[0m  0.1511\n",
      "     15  5353070993.2400  \u001b[32m4652992651.8794\u001b[0m  0.1482\n",
      "     16  \u001b[36m5332690207.8700\u001b[0m  \u001b[32m4649182128.4190\u001b[0m  0.1505\n",
      "     17  \u001b[36m5307296796.1356\u001b[0m  \u001b[32m4646323757.8628\u001b[0m  0.1516\n",
      "     18  5309153658.8121  \u001b[32m4643675892.0146\u001b[0m  0.1543\n",
      "     19  5317705337.9846  \u001b[32m4641257051.4922\u001b[0m  0.1514\n",
      "     20  \u001b[36m5299066815.2839\u001b[0m  \u001b[32m4639456850.9964\u001b[0m  0.1491\n",
      "a fold\n",
      "  epoch       train_loss        valid_loss     dur\n",
      "-------  ---------------  ----------------  ------\n",
      "      1  \u001b[36m6581091604.7074\u001b[0m  \u001b[32m10356016067.6274\u001b[0m  0.1604\n",
      "      2  \u001b[36m6401363794.1508\u001b[0m  \u001b[32m9970834945.1031\u001b[0m  0.1519\n",
      "      3  \u001b[36m5797373243.7885\u001b[0m  \u001b[32m9155496514.2698\u001b[0m  0.1501\n",
      "      4  \u001b[36m5028852162.4930\u001b[0m  \u001b[32m8440831351.7905\u001b[0m  0.1503\n",
      "      5  \u001b[36m4591300320.7055\u001b[0m  \u001b[32m8108486173.9105\u001b[0m  0.1511\n",
      "      6  \u001b[36m4392158779.2581\u001b[0m  \u001b[32m7952831938.5244\u001b[0m  0.1500\n",
      "      7  \u001b[36m4317505038.2364\u001b[0m  \u001b[32m7861005295.4538\u001b[0m  0.1507\n",
      "      8  \u001b[36m4238989842.6919\u001b[0m  \u001b[32m7793519416.0663\u001b[0m  0.1514\n",
      "      9  \u001b[36m4186698169.6244\u001b[0m  \u001b[32m7742594771.8979\u001b[0m  0.1504\n",
      "     10  \u001b[36m4145567964.1651\u001b[0m  \u001b[32m7702880740.2957\u001b[0m  0.1590\n",
      "     11  \u001b[36m4134884145.0953\u001b[0m  \u001b[32m7671679843.5532\u001b[0m  0.1615\n",
      "     12  \u001b[36m4103218260.4635\u001b[0m  \u001b[32m7647483020.6854\u001b[0m  0.1600\n",
      "     13  \u001b[36m4060384701.5919\u001b[0m  \u001b[32m7627351904.3076\u001b[0m  0.1608\n",
      "     14  \u001b[36m4058293655.9218\u001b[0m  \u001b[32m7613440508.6908\u001b[0m  0.1585\n",
      "     15  4061572257.0555  \u001b[32m7603653480.4322\u001b[0m  0.1557\n",
      "     16  \u001b[36m4023758366.7005\u001b[0m  \u001b[32m7591816165.6109\u001b[0m  0.1798\n",
      "     17  4035467395.8190  \u001b[32m7584629201.1826\u001b[0m  0.1517\n",
      "     18  4024141429.4765  \u001b[32m7579140149.0752\u001b[0m  0.1519\n",
      "     19  \u001b[36m4014562056.3594\u001b[0m  \u001b[32m7574631705.5406\u001b[0m  0.1499\n",
      "     20  \u001b[36m4002687031.4603\u001b[0m  \u001b[32m7569776811.6990\u001b[0m  0.1493\n",
      "a fold\n",
      "  epoch       train_loss        valid_loss     dur\n",
      "-------  ---------------  ----------------  ------\n",
      "      1  \u001b[36m6582938067.2753\u001b[0m  \u001b[32m10138668509.9741\u001b[0m  0.1666\n",
      "      2  \u001b[36m6409304547.9940\u001b[0m  \u001b[32m9761032462.0431\u001b[0m  0.1616\n",
      "      3  \u001b[36m5813669907.9436\u001b[0m  \u001b[32m8939846273.3576\u001b[0m  0.1556\n",
      "      4  \u001b[36m5031908919.5027\u001b[0m  \u001b[32m8187450395.2164\u001b[0m  0.1648\n",
      "      5  \u001b[36m4522972831.5279\u001b[0m  \u001b[32m7834175315.4896\u001b[0m  0.1659\n",
      "      6  \u001b[36m4333753784.1392\u001b[0m  \u001b[32m7699682251.7096\u001b[0m  0.1544\n",
      "      7  \u001b[36m4206350157.1437\u001b[0m  \u001b[32m7619865796.3275\u001b[0m  0.1621\n",
      "      8  \u001b[36m4200138179.1719\u001b[0m  \u001b[32m7569912613.3775\u001b[0m  0.1536\n",
      "      9  \u001b[36m4132003856.8884\u001b[0m  \u001b[32m7532551978.9884\u001b[0m  0.1568\n",
      "     10  \u001b[36m4124901502.5573\u001b[0m  \u001b[32m7506990249.4611\u001b[0m  0.1587\n",
      "     11  \u001b[36m4090985492.2619\u001b[0m  \u001b[32m7486980129.6016\u001b[0m  0.1512\n",
      "     12  \u001b[36m4063245164.9687\u001b[0m  \u001b[32m7472133731.5532\u001b[0m  0.1506\n",
      "     13  4081255007.5385  \u001b[32m7463554626.5244\u001b[0m  0.1508\n",
      "     14  4077547975.5001  \u001b[32m7457102002.1372\u001b[0m  0.1548\n",
      "     15  \u001b[36m4045282857.9877\u001b[0m  \u001b[32m7451380692.5555\u001b[0m  0.1481\n",
      "     16  4045812706.3816  \u001b[32m7446839460.9214\u001b[0m  0.1476\n",
      "     17  \u001b[36m4020632394.3007\u001b[0m  \u001b[32m7442401598.5469\u001b[0m  0.1489\n",
      "     18  4043268516.3441  \u001b[32m7439660272.6735\u001b[0m  0.1532\n",
      "     19  4021710821.4155  \u001b[32m7437026515.1448\u001b[0m  0.1586\n",
      "     20  4022585950.1170  \u001b[32m7433947386.3255\u001b[0m  0.1621\n",
      "Ensemble CV RMSE score: 70945.74954 ± 10242.04453\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from skorch import NeuralNetRegressor\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "import pandas as pd\n",
    "\n",
    "# Define PyTorch model with more capacity\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)  # Increased to 128 neurons\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)  # Increased second layer to 64 neurons\n",
    "        self.fc3 = nn.Linear(64, 1)  # Output layer remains the same\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout to prevent overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x.squeeze(-1)  # Remove the last dimension\n",
    "\n",
    "# Function to train CatBoost model\n",
    "def train_catboost_model(X_train, y_train):\n",
    "    catboost_model = CatBoostRegressor(iterations=500, depth=6, learning_rate=0.1, loss_function='RMSE', silent=True)\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    return catboost_model\n",
    "\n",
    "# Function to create the ensemble\n",
    "def create_ensemble(X_train, y_train, X_test, y_test, input_size, epochs=20, batch_size=64):\n",
    "    # Convert data to float32 for PyTorch compatibility\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)  # Ensure target values are also float32\n",
    "    y_test = y_test.astype(np.float32)\n",
    "\n",
    "    # Wrap PyTorch model with skorch\n",
    "    pytorch_model = NeuralNetRegressor(\n",
    "        module=SimpleNN,\n",
    "        module__input_size=input_size,\n",
    "        max_epochs=epochs,\n",
    "        lr=0.001,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optim.Adam,\n",
    "        criterion=nn.MSELoss,\n",
    "        device='cpu'  # Use 'cuda' if you have a GPU available\n",
    "    )\n",
    "\n",
    "    # Train Ridge model\n",
    "    ridge_model = Ridge(alpha=1.0)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "\n",
    "    # Train CatBoost model\n",
    "    catboost_model = train_catboost_model(X_train, y_train)\n",
    "\n",
    "    # Create the ensemble using VotingRegressor\n",
    "    ensemble = VotingRegressor([\n",
    "        ('nn', pytorch_model),\n",
    "        ('ridge', ridge_model),\n",
    "        ('catboost', catboost_model)\n",
    "    ])\n",
    "    \n",
    "    # Train the ensemble\n",
    "    ensemble.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    # Predict on the test set\n",
    "    predictions = np.column_stack([\n",
    "        est.predict(X_test).squeeze() if hasattr(est, 'predict') else est.predict(X_test)\n",
    "        for name, est in ensemble.named_estimators_.items()\n",
    "    ])\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions.mean(axis=1)))\n",
    "\n",
    "    # Predict on the test set\n",
    "    #predictions = ensemble.predict(X_test)\n",
    "    \n",
    "    # Calculate RMSE for the ensemble\n",
    "    #rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "# Function to train PyTorch model (using skorch wrapper)\n",
    "def train_pytorch_model(X_train, y_train, input_size, epochs=20, batch_size=64):\n",
    "    X_train = X_train.astype(np.float32)  # Convert data to float32\n",
    "    y_train = y_train.astype(np.float32)  # Convert target to float32\n",
    "    pytorch_model = NeuralNetRegressor(\n",
    "        module=SimpleNN,\n",
    "        module__input_size=input_size,\n",
    "        max_epochs=epochs,\n",
    "        lr=0.001,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optim.Adam,\n",
    "        criterion=nn.MSELoss,\n",
    "        device='cpu'  # Use 'cuda' if you have a GPU available\n",
    "    )\n",
    "    pytorch_model.fit(X_train, y_train)\n",
    "    return pytorch_model\n",
    "\n",
    "# Cross-validation setup\n",
    "def cross_validate_ensemble(X, y, target_encoder, scaler, n_splits=5, n_repeats=1, epochs=20, batch_size=64):\n",
    "    kfold = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=SEED)\n",
    "    rmse_scores_ensemble = []\n",
    "\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        print('a fold')\n",
    "        # Split the data\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Preprocess the data\n",
    "        X_train_scaled, nan_columns = preprocess_data(X_train, y_train, target_encoder, scaler)\n",
    "        X_test_scaled, _ = preprocess_data(X_test, target_encoder=target_encoder, scaler=scaler, nan_columns=nan_columns)\n",
    "\n",
    "        # Train and evaluate the ensemble\n",
    "        rmse_ensemble = create_ensemble(X_train_scaled, y_train, X_test_scaled, y_test, input_size=X_train_scaled.shape[1], epochs=epochs, batch_size=batch_size)\n",
    "        rmse_scores_ensemble.append(rmse_ensemble)\n",
    "\n",
    "    print(f'Ensemble CV RMSE score: {np.mean(rmse_scores_ensemble):.5f} ± {np.std(rmse_scores_ensemble):.5f}')\n",
    "\n",
    "# Prepare data\n",
    "SEED = 5\n",
    "DEV_FRACTION = .1\n",
    "df_dev = train_df_full.sample(frac=DEV_FRACTION, random_state=SEED)\n",
    "\n",
    "print(df_dev.shape)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_dev.drop(columns='price')\n",
    "y = df_dev['price']\n",
    "\n",
    "# Cross-validation setup\n",
    "kfold = RepeatedKFold(n_splits=5, n_repeats=1, random_state=SEED)\n",
    "\n",
    "# Initialize TargetEncoder and StandardScaler\n",
    "target_encoder = TargetEncoder(random_state=SEED, target_type='continuous').set_output(transform='pandas')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Run cross-validation\n",
    "cross_validate_ensemble(X, y, target_encoder, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with params: {'epochs': [20, 30], 'batch_size': [64, 128], 'learning_rate': [0.001, 0.0005], 'hidden1_size': [128, 256], 'hidden2_size': [64, 128], 'dropout_prob': [0.3, 0.5]}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "batch_size should be a positive integer value, but got batch_size=[64, 128]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 128\u001b[0m\n\u001b[1;32m    116\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    117\u001b[0m     {\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m],               \u001b[38;5;66;03m# Number of epochs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m     }\n\u001b[1;32m    125\u001b[0m ]\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Run hyperparameter search\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m best_params, best_score \u001b[38;5;241m=\u001b[39m hyperparameter_search(X, y, target_encoder, scaler, param_grid)\n",
      "Cell \u001b[0;32mIn[5], line 87\u001b[0m, in \u001b[0;36mhyperparameter_search\u001b[0;34m(X, y, target_encoder, scaler, param_grid)\u001b[0m\n\u001b[1;32m     84\u001b[0m X_test_scaled, _ \u001b[38;5;241m=\u001b[39m preprocess_data(X_test, target_encoder\u001b[38;5;241m=\u001b[39mtarget_encoder, scaler\u001b[38;5;241m=\u001b[39mscaler, nan_columns\u001b[38;5;241m=\u001b[39mnan_columns)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Train and evaluate PyTorch model with current hyperparameters\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m model \u001b[38;5;241m=\u001b[39m train_pytorch_model(X_train_scaled, y_train, input_size\u001b[38;5;241m=\u001b[39mX_train_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m     88\u001b[0m rmse_nn \u001b[38;5;241m=\u001b[39m evaluate_pytorch_model(model, X_test_scaled, y_test)\n\u001b[1;32m     89\u001b[0m rmse_scores_nn\u001b[38;5;241m.\u001b[39mappend(rmse_nn)\n",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m, in \u001b[0;36mtrain_pytorch_model\u001b[0;34m(X_train, y_train, input_size, epochs, batch_size, learning_rate, hidden1_size, hidden2_size, dropout_prob)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Create DataLoader\u001b[39;00m\n\u001b[1;32m     36\u001b[0m train_data \u001b[38;5;241m=\u001b[39m TensorDataset(X_train_tensor, y_train_tensor)\n\u001b[0;32m---> 37\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Initialize PyTorch model, loss, and optimizer\u001b[39;00m\n\u001b[1;32m     40\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleNN(input_size\u001b[38;5;241m=\u001b[39minput_size, hidden1_size\u001b[38;5;241m=\u001b[39mhidden1_size, hidden2_size\u001b[38;5;241m=\u001b[39mhidden2_size, dropout_prob\u001b[38;5;241m=\u001b[39mdropout_prob)\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/torch/utils/data/dataloader.py:357\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    353\u001b[0m             sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m batch_sampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# auto_collation without custom batch_sampler\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     batch_sampler \u001b[38;5;241m=\u001b[39m BatchSampler(sampler, batch_size, drop_last)\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_last \u001b[38;5;241m=\u001b[39m drop_last\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/torch/utils/data/sampler.py:268\u001b[0m, in \u001b[0;36mBatchSampler.__init__\u001b[0;34m(self, sampler, batch_size, drop_last)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sampler: Union[Sampler[\u001b[38;5;28mint\u001b[39m], Iterable[\u001b[38;5;28mint\u001b[39m]], batch_size: \u001b[38;5;28mint\u001b[39m, drop_last: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# Since collections.abc.Iterable does not check for `__getitem__`, which\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# is one way for an object to be an iterable, we don't do an `isinstance`\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# check here.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_size, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_size, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m    267\u001b[0m             batch_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 268\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size should be a positive integer value, but got batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(drop_last, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_last should be a boolean value, but got drop_last=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdrop_last\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: batch_size should be a positive integer value, but got batch_size=[64, 128]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "# Define PyTorch model with hyperparameters for tuning\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden1_size=128, hidden2_size=64, dropout_prob=0.3):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.fc3 = nn.Linear(hidden2_size, 1)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x.squeeze(-1)  # Remove the last dimension\n",
    "\n",
    "# Function to train PyTorch model with tunable hyperparameters\n",
    "def train_pytorch_model(X_train, y_train, input_size, epochs=20, batch_size=64, learning_rate=0.001, hidden1_size=128, hidden2_size=64, dropout_prob=0.3):\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize PyTorch model, loss, and optimizer\n",
    "    model = SimpleNN(input_size=input_size, hidden1_size=hidden1_size, hidden2_size=hidden2_size, dropout_prob=dropout_prob)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model\n",
    "\n",
    "# Function to evaluate PyTorch model\n",
    "def evaluate_pytorch_model(model, X_test, y_test):\n",
    "    # Convert test data to PyTorch tensors\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor)\n",
    "        rmse = torch.sqrt(torch.mean((predictions - y_test_tensor) ** 2)).item()\n",
    "    return rmse\n",
    "\n",
    "# Hyperparameter optimization loop\n",
    "def hyperparameter_search(X, y, target_encoder, scaler, param_grid):\n",
    "    best_params = None\n",
    "    best_score = float('inf')\n",
    "    for params in param_grid:\n",
    "        print(f\"Testing with params: {params}\")\n",
    "        rmse_scores_nn = []\n",
    "\n",
    "        # Cross-validation\n",
    "        kfold = RepeatedKFold(n_splits=5, n_repeats=1, random_state=SEED)\n",
    "        for train_index, test_index in kfold.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            # Preprocess the data\n",
    "            X_train_scaled, nan_columns = preprocess_data(X_train, y_train, target_encoder, scaler)\n",
    "            X_test_scaled, _ = preprocess_data(X_test, target_encoder=target_encoder, scaler=scaler, nan_columns=nan_columns)\n",
    "\n",
    "            # Train and evaluate PyTorch model with current hyperparameters\n",
    "            model = train_pytorch_model(X_train_scaled, y_train, input_size=X_train_scaled.shape[1], **params)\n",
    "            rmse_nn = evaluate_pytorch_model(model, X_test_scaled, y_test)\n",
    "            rmse_scores_nn.append(rmse_nn)\n",
    "\n",
    "        mean_rmse = np.mean(rmse_scores_nn)\n",
    "        print(f\"Mean RMSE for params {params}: {mean_rmse:.5f}\")\n",
    "        \n",
    "        # Update best score and parameters\n",
    "        if mean_rmse < best_score:\n",
    "            best_score = mean_rmse\n",
    "            best_params = params\n",
    "\n",
    "    print(f\"Best RMSE: {best_score:.5f} with params: {best_params}\")\n",
    "    return best_params, best_score\n",
    "\n",
    "# Prepare data\n",
    "SEED = 4\n",
    "DEV_FRACTION = .5\n",
    "df_dev = train_df_full.sample(frac=DEV_FRACTION, random_state=SEED)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_dev.drop(columns='price')\n",
    "y = df_dev['price']\n",
    "\n",
    "# Initialize TargetEncoder and StandardScaler\n",
    "target_encoder = TargetEncoder(random_state=SEED, target_type='continuous').set_output(transform='pandas')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define parameter grid for hyperparameter search\n",
    "param_grid = [\n",
    "    {\n",
    "        'epochs': [20, 30],               # Number of epochs\n",
    "        'batch_size': [64, 128],          # Batch size\n",
    "        'learning_rate': [0.001, 0.0005], # Learning rate\n",
    "        'hidden1_size': [128, 256],       # First hidden layer size\n",
    "        'hidden2_size': [64, 128],        # Second hidden layer size\n",
    "        'dropout_prob': [0.3, 0.5]        # Dropout probability\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run hyperparameter search\n",
    "best_params, best_score = hyperparameter_search(X, y, target_encoder, scaler, param_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# above the hyper param needs to implement the lists into singles\n",
    "# here, we are doing another submission for blended model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch       train_loss       valid_loss     dur\n",
      "-------  ---------------  ---------------  ------\n",
      "      1  \u001b[36m6289850170.4341\u001b[0m  \u001b[32m5186077151.2727\u001b[0m  1.8258\n",
      "      2  \u001b[36m5476816694.6605\u001b[0m  \u001b[32m5110998660.9188\u001b[0m  1.8278\n",
      "      3  \u001b[36m5436950200.7715\u001b[0m  \u001b[32m5089125104.3781\u001b[0m  1.7541\n",
      "      4  \u001b[36m5424680796.8710\u001b[0m  \u001b[32m5079573866.3934\u001b[0m  1.7917\n",
      "      5  \u001b[36m5411299301.0219\u001b[0m  \u001b[32m5075067791.4929\u001b[0m  1.7567\n",
      "      6  \u001b[36m5409756922.9280\u001b[0m  \u001b[32m5072396486.5329\u001b[0m  1.7667\n",
      "      7  \u001b[36m5405006535.7576\u001b[0m  \u001b[32m5071179457.2645\u001b[0m  1.8669\n",
      "      8  \u001b[36m5401908786.4410\u001b[0m  \u001b[32m5070165521.7096\u001b[0m  1.7954\n",
      "      9  5404455016.4331  \u001b[32m5069738445.3051\u001b[0m  1.8207\n",
      "     10  5403479674.0573  \u001b[32m5069055797.8553\u001b[0m  1.7612\n",
      "     11  5404216668.7929  \u001b[32m5068479951.0669\u001b[0m  1.6904\n",
      "     12  \u001b[36m5398717384.3237\u001b[0m  \u001b[32m5068327056.0666\u001b[0m  1.6903\n",
      "     13  5402430333.7523  \u001b[32m5067968267.2463\u001b[0m  1.8123\n",
      "     14  5406663231.8027  \u001b[32m5067631694.7716\u001b[0m  1.7337\n",
      "     15  \u001b[36m5397039569.6025\u001b[0m  \u001b[32m5067364400.3628\u001b[0m  1.7398\n",
      "     16  5399562416.8514  \u001b[32m5067130723.2410\u001b[0m  1.6901\n",
      "     17  5403133463.1667  \u001b[32m5066912036.3935\u001b[0m  1.7162\n",
      "     18  5400754948.1419  \u001b[32m5066673057.5114\u001b[0m  1.8244\n",
      "     19  \u001b[36m5396579693.1992\u001b[0m  \u001b[32m5066537363.7260\u001b[0m  1.7540\n",
      "     20  5401085453.7933  \u001b[32m5066372285.0688\u001b[0m  1.7426\n",
      "Submission file created as 'submission10_Blended_Ensemble.csv'\n"
     ]
    }
   ],
   "source": [
    "# Main function to train the ensemble on full data and make a submission\n",
    "def train_full_model_and_submit(train_df, test_df, target_encoder, scaler, epochs=20, batch_size=64):\n",
    "    # Separate features and target\n",
    "    X_train = train_df.drop(columns='price')\n",
    "    y_train = train_df['price']\n",
    "    X_test = test_df\n",
    "\n",
    "    # Preprocess the data\n",
    "    X_train_scaled, nan_columns = preprocess_data(X_train, y_train, target_encoder, scaler)\n",
    "    X_test_scaled, _ = preprocess_data(X_test, target_encoder=target_encoder, scaler=scaler, nan_columns=nan_columns)\n",
    "\n",
    "    # Train the ensemble model (PyTorch, Ridge, and CatBoost)\n",
    "    X_train_scaled = X_train_scaled.astype(np.float32)  # Convert to float32 for PyTorch\n",
    "    y_train = y_train.astype(np.float32)  # Ensure y_train is also float32\n",
    "    ensemble = VotingRegressor([\n",
    "        ('nn', NeuralNetRegressor(\n",
    "            module=SimpleNN,\n",
    "            module__input_size=X_train_scaled.shape[1],\n",
    "            max_epochs=epochs,\n",
    "            lr=0.001,\n",
    "            batch_size=batch_size,\n",
    "            optimizer=optim.Adam,\n",
    "            criterion=nn.MSELoss,\n",
    "            device='cpu'  # Use 'cuda' if you have a GPU available\n",
    "        )),\n",
    "        ('ridge', Ridge(alpha=1.0)),\n",
    "        ('catboost', CatBoostRegressor(\n",
    "            iterations=500, depth=6, learning_rate=0.1, loss_function='RMSE', silent=True\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train the ensemble on the full training data\n",
    "    ensemble.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    X_test_scaled = X_test_scaled.astype(np.float32)  # Ensure X_test is also float32\n",
    "    predictions = ensemble.predict(X_test_scaled)\n",
    "\n",
    "    # Create the submission DataFrame\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'price': predictions\n",
    "    })\n",
    "\n",
    "    # Save the submission to CSV\n",
    "    submission.to_csv('submission10_Blended_Ensemble.csv', index=False)\n",
    "\n",
    "    print(\"Submission file created as 'submission10_Blended_Ensemble.csv'\")\n",
    "\n",
    "# Train the full model and create a submission\n",
    "train_full_model_and_submit(train_df_full, test_df_full, target_encoder, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
